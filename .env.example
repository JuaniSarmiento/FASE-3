# ============================================================================
# AI-Native MVP - Environment Variables Configuration
# ============================================================================
# Copy this file to .env and fill in your actual values
# IMPORTANT: Never commit .env to version control!

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================

# Which LLM provider to use: "mock", "openai", "anthropic", "gemini"
# - "mock": No API calls, uses predefined responses (default, free)
# - "openai": OpenAI GPT-4/GPT-3.5 (requires API key)
# - "anthropic": Claude (requires API key)
# - "gemini": Google Gemini 1.5 Pro/Flash (requires API key)
LLM_PROVIDER=mock

# ============================================================================
# OPENAI CONFIGURATION (only needed if LLM_PROVIDER=openai)
# ============================================================================

# Your OpenAI API key (get from: https://platform.openai.com/api-keys)
# Example: sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_API_KEY=

# Model to use (options: gpt-4, gpt-4-turbo, gpt-3.5-turbo)
# Recommended: gpt-4 for best quality, gpt-3.5-turbo for cost efficiency
OPENAI_MODEL=gpt-4

# Organization ID (optional, only if using multiple organizations)
OPENAI_ORGANIZATION=

# Temperature for text generation (0.0 = deterministic, 1.0 = creative)
# Recommended: 0.7 for balanced responses
OPENAI_TEMPERATURE=0.7

# Maximum tokens per response (optional, leave empty for model default)
# GPT-4: max 8192, GPT-3.5: max 4096
OPENAI_MAX_TOKENS=

# ============================================================================
# ANTHROPIC CONFIGURATION (only needed if LLM_PROVIDER=anthropic)
# ============================================================================

# Your Anthropic API key (get from: https://console.anthropic.com/settings/keys)
ANTHROPIC_API_KEY=

# Model to use (options: claude-3-opus, claude-3-sonnet, claude-3-haiku)
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# ============================================================================
# GOOGLE GEMINI CONFIGURATION (only needed if LLM_PROVIDER=gemini)
# ============================================================================

# Your Google API key (get from: https://makersuite.google.com/app/apikey)
# Example: AIzaSyXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
GEMINI_API_KEY=

# Model to use (options: gemini-1.5-pro, gemini-1.5-flash, gemini-pro)
# Recommended: gemini-1.5-flash for speed and cost, gemini-1.5-pro for quality
# gemini-1.5-flash: Fast, economical, 2M token context window
# gemini-1.5-pro: High quality, 2M token context window
GEMINI_MODEL=gemini-1.5-flash

# Temperature for text generation (0.0 = deterministic, 1.0 = creative)
# Recommended: 0.7 for balanced responses
GEMINI_TEMPERATURE=0.7

# Maximum tokens per response (optional, leave empty for model default)
# Gemini 1.5: max 8192 output tokens
GEMINI_MAX_TOKENS=

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# Database URL (SQLite for dev, PostgreSQL for production)
# Examples:
#   SQLite: sqlite:///ai_native.db
#   PostgreSQL: postgresql://user:password@localhost:5432/ai_native
DATABASE_URL=sqlite:///ai_native.db

# Database Connection Pool Configuration (P1.3 - Production Readiness)
# Only applies to PostgreSQL (SQLite ignores these settings)
# Pool size: Number of permanent connections to keep open
# Recommended: 20 for API servers with 4-8 workers
DB_POOL_SIZE=20

# Max overflow: Additional connections beyond pool_size (created on demand)
# Total max connections = pool_size + max_overflow
# Recommended: 40 (60 total connections max with default pool_size)
DB_MAX_OVERFLOW=40

# Pool timeout: Seconds to wait for an available connection before giving up
# Recommended: 30 seconds
DB_POOL_TIMEOUT=30

# Pool recycle: Seconds before recycling connections (prevents stale connections)
# Recommended: 3600 (1 hour) - MySQL requires <8h, PostgreSQL can be longer
DB_POOL_RECYCLE=3600

# ============================================================================
# API SERVER CONFIGURATION
# ============================================================================

# API server host (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
API_HOST=127.0.0.1

# API server port
API_PORT=8000

# Enable debug mode (true/false)
# WARNING: Never enable in production!
DEBUG=true

# Environment (development, staging, production)
ENVIRONMENT=development

# ============================================================================
# CORS CONFIGURATION
# ============================================================================

# CORS allowed origins (comma-separated URLs without trailing slashes)
# Development example: http://localhost:3000,http://localhost:5173,http://localhost:8080
# Production example: https://app.example.com,https://www.example.com
# IMPORTANT: In production, ONLY include your actual frontend domains!
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:8080

# ============================================================================
# LLM CACHE CONFIGURATION
# ============================================================================

# Enable LLM response cache to reduce API costs (true/false)
# Recommended: true (can save 30-50% on LLM API calls for repeated prompts)
LLM_CACHE_ENABLED=true

# Cache backend: "memory" (default, single instance) or "redis" (distributed, production)
# - memory: Fast, simple, but lost on restart and not shared between workers
# - redis: Persistent, shared across workers/pods, recommended for production
LLM_CACHE_BACKEND=memory

# Redis connection URL (only needed if LLM_CACHE_BACKEND=redis)
# Format: redis://[:password@]host:port[/db]
# Examples:
#   - Local: redis://localhost:6379/0
#   - Docker: redis://ai-native-redis:6379/0
#   - AWS ElastiCache: redis://cache.example.com:6379/0
#   - With password: redis://:mypassword@localhost:6379/0
#   - Redis Sentinel: redis-sentinel://host1:26379,host2:26379/mymaster/0
REDIS_URL=redis://localhost:6379/0

# Cache salt for security (CRITICAL: MUST be unique per institution)
# Prevents cache poisoning attacks and cross-student cache leakage
# REQUIRED: Generate with: python -c "import secrets; print(secrets.token_hex(32))"
# Example (DO NOT USE THIS IN PRODUCTION):
CACHE_SALT=CHANGE_THIS_TO_A_SECURE_RANDOM_VALUE_GENERATED_WITH_COMMAND_ABOVE

# Cache TTL (Time To Live) in seconds
# How long cached responses remain valid before expiring
# Default: 3600 (1 hour)
# Recommended values:
#   - 1800 (30 min): For frequently changing content
#   - 3600 (1 hour): Balanced (default)
#   - 7200 (2 hours): For stable educational content
#   - 86400 (24 hours): For very stable content (not recommended for tutoring)
LLM_CACHE_TTL=3600

# Maximum number of cached entries (LRU eviction when full)
# Default: 1000 entries
# Memory estimate: ~100KB per entry average = ~100MB total at 1000 entries
# Recommended values:
#   - 500: Low memory environments
#   - 1000: Default, balanced
#   - 5000: High traffic, more memory available
# Note: With Redis, this limit is advisory (Redis manages memory separately)
LLM_CACHE_MAX_ENTRIES=1000

# ============================================================================
# SECURITY & AUTHENTICATION CONFIGURATION (Production Readiness - P1.1)
# ============================================================================

# SECRET KEY for JWT tokens
# CRITICAL: Generate a secure random key for production!
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
# Or: openssl rand -hex 32
# REQUIRED: This variable MUST be set (no default value)
# Example (DO NOT USE THIS IN PRODUCTION):
JWT_SECRET_KEY=CHANGE_THIS_TO_A_SECURE_RANDOM_VALUE_GENERATED_WITH_COMMAND_ABOVE

# JWT Algorithm (HS256 recommended for symmetric signing)
JWT_ALGORITHM=HS256

# Access token expiration time in MINUTES
# Recommended: 15-60 minutes for security
# Default: 30 minutes
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30

# Refresh token expiration time in DAYS
# Recommended: 7-30 days
# Default: 7 days
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log format (json, text)
LOG_FORMAT=text

# ============================================================================
# GOVERNANCE CONFIGURATION
# ============================================================================

# Maximum AI involvement level allowed (0.0 to 1.0)
# 0.0 = No AI help, 1.0 = Full AI assistance
MAX_AI_INVOLVEMENT=0.8

# Block complete code solutions without mediation (true/false)
BLOCK_COMPLETE_SOLUTIONS=true

# Require explicit traceability for all interactions (true/false)
REQUIRE_TRACEABILITY=true

# ============================================================================
# FEATURE FLAGS
# ============================================================================

# Enable N4 cognitive traceability (true/false)
ENABLE_N4_TRACEABILITY=true

# Enable risk analysis (true/false)
ENABLE_RISK_ANALYSIS=true

# Enable process evaluation (true/false)
ENABLE_PROCESS_EVALUATION=true

# Enable Git integration for N2 traceability (true/false)
ENABLE_GIT_INTEGRATION=false